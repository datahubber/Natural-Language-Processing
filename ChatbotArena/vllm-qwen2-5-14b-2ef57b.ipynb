{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1bdcdc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T15:59:37.450630Z",
     "iopub.status.busy": "2025-02-03T15:59:37.449904Z",
     "iopub.status.idle": "2025-02-03T15:59:37.458664Z",
     "shell.execute_reply": "2025-02-03T15:59:37.457897Z"
    },
    "papermill": {
     "duration": 0.014569,
     "end_time": "2025-02-03T15:59:37.460231",
     "exception": false,
     "start_time": "2025-02-03T15:59:37.445662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing install_packages.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile install_packages.py\n",
    "\n",
    "def install_packages():\n",
    "  import os\n",
    "  os.system('pip uninstall -y torch')\n",
    "  os.system('pip uninstall -y torchvision')\n",
    "  os.system('pip install -q --no-index --find-links=/kaggle/input/0-6-3-post1-wheels-vllm vllm')\n",
    "  os.system('pip install -q --no-index -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl')\n",
    "  os.system('pip install -q --no-index -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl')\n",
    "  os.system('pip uninstall -y pynvml')\n",
    "  os.system('pip install --no-deps --no-index /kaggle/input/0-6-3-post1-wheels-vllm/nvidia_ml_py-12.560.30-py3-none-any.whl')\n",
    "  os.system('pip install --no-deps --no-index /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl')\n",
    "\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2eecc4c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T15:59:37.466613Z",
     "iopub.status.busy": "2025-02-03T15:59:37.466377Z",
     "iopub.status.idle": "2025-02-03T16:02:26.080983Z",
     "shell.execute_reply": "2025-02-03T16:02:26.079892Z"
    },
    "papermill": {
     "duration": 168.619803,
     "end_time": "2025-02-03T16:02:26.082902",
     "exception": false,
     "start_time": "2025-02-03T15:59:37.463099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing packages\n",
      "Found existing installation: torch 2.4.0\r\n",
      "Uninstalling torch-2.4.0:\r\n",
      "  Successfully uninstalled torch-2.4.0\r\n",
      "Found existing installation: torchvision 0.19.0\r\n",
      "Uninstalling torchvision-0.19.0:\r\n",
      "  Successfully uninstalled torchvision-0.19.0\r\n",
      "Found existing installation: pynvml 11.4.1\r\n",
      "Uninstalling pynvml-11.4.1:\r\n",
      "  Successfully uninstalled pynvml-11.4.1\r\n",
      "Processing /kaggle/input/0-6-3-post1-wheels-vllm/nvidia_ml_py-12.560.30-py3-none-any.whl\r\n",
      "Installing collected packages: nvidia-ml-py\r\n",
      "  Attempting uninstall: nvidia-ml-py\r\n",
      "    Found existing installation: nvidia-ml-py 11.495.46\r\n",
      "    Uninstalling nvidia-ml-py-11.495.46:\r\n",
      "      Successfully uninstalled nvidia-ml-py-11.495.46\r\n",
      "Successfully installed nvidia-ml-py-12.560.30\r\n",
      "Processing /kaggle/input/logits-processor-zoo/logits_processor_zoo-0.1.0-py3-none-any.whl\r\n",
      "Installing collected packages: logits-processor-zoo\r\n",
      "Successfully installed logits-processor-zoo-0.1.0\r\n",
      "---------------------------------------- \n",
      "vllm version= 0.6.3.post1\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import vllm\n",
    "    print('vllm version=',vllm.__version__)\n",
    "except ImportError:\n",
    "    print('Installing packages')\n",
    "    !python install_packages.py\n",
    "    import vllm\n",
    "    print('----'*10,'\\nvllm version=',vllm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9031cd88",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-03T16:02:26.091252Z",
     "iopub.status.busy": "2025-02-03T16:02:26.090666Z",
     "iopub.status.idle": "2025-02-03T16:02:26.094980Z",
     "shell.execute_reply": "2025-02-03T16:02:26.094247Z"
    },
    "papermill": {
     "duration": 0.01014,
     "end_time": "2025-02-03T16:02:26.096632",
     "exception": false,
     "start_time": "2025-02-03T16:02:26.086492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "\n",
    "run_eval = True ## To check score on train data- 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe52a4d",
   "metadata": {
    "papermill": {
     "duration": 0.003017,
     "end_time": "2025-02-03T16:02:26.102927",
     "exception": false,
     "start_time": "2025-02-03T16:02:26.099910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e77e5ff2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T16:02:26.111781Z",
     "iopub.status.busy": "2025-02-03T16:02:26.111526Z",
     "iopub.status.idle": "2025-02-03T16:02:26.120104Z",
     "shell.execute_reply": "2025-02-03T16:02:26.119284Z"
    },
    "papermill": {
     "duration": 0.015684,
     "end_time": "2025-02-03T16:02:26.121752",
     "exception": false,
     "start_time": "2025-02-03T16:02:26.106068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run_vllm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile run_vllm.py\n",
    "system_prompt='''You are a skilled judge evaluating responses from two large language models(LLMs). Your task is to select the response that best meets the user's needs based on the query provided.\n",
    "\n",
    "**Input Format:**\n",
    "<Query>\n",
    "[User's original query to both LLMs]\n",
    "</Query>\n",
    "\n",
    "<Response_A>\n",
    "[First LLM's response]\n",
    "</Response_A>\n",
    "\n",
    "<Response_B>\n",
    "[Second LLM's response]\n",
    "</Response_B>\n",
    "\n",
    "**Your Task:**\n",
    "Carefully analyze both <Response_A> and <Response_B> in relation to the Query. Determine which response is more likely to be selected by a user based on the following criteria:\n",
    "- Completeness in addressing the query\n",
    "- Accuracy of information\n",
    "- Clarity and coherence\n",
    "- Conciseness vs appropriate detail\n",
    "- Helpful examples or explanations when needed\n",
    "- Professional yet engaging tone\n",
    "- Sound reasoning and logic\n",
    "- Format and presentation\n",
    "\n",
    "'''\n",
    "\n",
    "system_prompt_chatformat='''system\n",
    "You are a skilled judge evaluating responses from two large language models(LLMs). Your task is to select the response that best meets the user's needs based on the query provided.\n",
    "\n",
    "**Input Format:**\n",
    "<Query>\n",
    "[User's original query to both LLMs]\n",
    "</Query>\n",
    "\n",
    "<Response_A>\n",
    "[First LLM's response]\n",
    "</Response_A>\n",
    "\n",
    "<Response_B>\n",
    "[Second LLM's response]\n",
    "</Response_B>\n",
    "\n",
    "**Your Task:**\n",
    "Carefully analyze both <Response_A> and <Response_B> in relation to the Query. Determine which response is more likely to be selected by a user based on the following criteria:\n",
    "- Completeness in addressing the query\n",
    "- Accuracy of information\n",
    "- Clarity and coherence\n",
    "- Conciseness vs appropriate detail\n",
    "- Helpful examples or explanations when needed\n",
    "- Professional yet engaging tone\n",
    "- Sound reasoning and logic\n",
    "- Format and presentation\n",
    "\n",
    "'''\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import gc\n",
    "import vllm\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "\n",
    "def count_numbers_in_text(text):\n",
    "    \"\"\"Count the number of digits/numbers in a text string\"\"\"\n",
    "    return len(re.findall(r'\\d+', text))\n",
    "\n",
    "def winner_with_postprocessing(logprob_dict, row, a_tok_id, b_tok_id, threshold=0.05):\n",
    "    \"\"\"Determine winner with post-processing logic\"\"\"\n",
    "    try:\n",
    "        if a_tok_id not in logprob_dict or b_tok_id not in logprob_dict:\n",
    "            return 'A' if a_tok_id in logprob_dict else 'B'\n",
    "            \n",
    "        a_logit = logprob_dict[a_tok_id].logprob\n",
    "        b_logit = logprob_dict[b_tok_id].logprob\n",
    "        \n",
    "        # If difference is smaller than threshold, count numbers\n",
    "        if abs(a_logit - b_logit) < threshold:\n",
    "            numbers_in_a = count_numbers_in_text(row['response_a'])\n",
    "            numbers_in_b = count_numbers_in_text(row['response_b'])\n",
    "            \n",
    "            if numbers_in_a != numbers_in_b:\n",
    "                return 'A' if numbers_in_a > numbers_in_b else 'B'\n",
    "            else:\n",
    "                # If number counts are equal, return based on logprobs\n",
    "                return 'A' if a_logit > b_logit else 'B'\n",
    "        else:\n",
    "            # If difference is larger than threshold, use logprobs directly\n",
    "            return 'A' if a_logit > b_logit else 'B'\n",
    "    except:\n",
    "        return 'A'  # Default fallback\n",
    "\n",
    "IS_SUBMISSION = bool(os.getenv(\"KAGGLE_IS_COMPETITION_RERUN\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "if IS_SUBMISSION:\n",
    "    df = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/test.parquet')\n",
    "else:\n",
    "    df = pd.read_parquet('/kaggle/input/wsdm-cup-multilingual-chatbot-arena/train.parquet')\n",
    "    df = df.sample(100, random_state=42).reset_index(drop=True).copy()\n",
    "    df['winner_GT'] = df['winner']\n",
    "    print('Length of df=',len(df))\n",
    "\n",
    "def preprocess_function_truncation(row, tokenizer,max_length,system_prompt,system_prompt_chatformat):\n",
    "    dot_tokens = tokenizer(\"......\", add_special_tokens=False)[\"input_ids\"]\n",
    "    system_token= [tokenizer.bos_token_id]+tokenizer(system_prompt_chatformat,add_special_tokens=False) [\"input_ids\"]+[tokenizer.eos_token_id]\n",
    "    user_token=tokenizer(\"user\\n\", add_special_tokens=False)[\"input_ids\"]\n",
    "    final_p_tokens = tokenizer('Which response is more likely to be selected by a user? (A or B)\\n', add_special_tokens=False)[\"input_ids\"]\n",
    "    p=row['prompt']\n",
    "    ra= row['response_a']\n",
    "    rb =row['response_b']\n",
    "    one_input_ids =system_token\n",
    "    prev_tokens_num =  len(system_token)+len(final_p_tokens)+len(user_token)\n",
    "    prompt=f'''**Here is your input to process now-**\\nInput:\\n\\n<Query>\\n{p}'''\n",
    "    response_a=f'''\\n<Response_A>\\n{ra}'''\n",
    "    response_b=f'''\\n<Response_B>\\n{rb}'''\n",
    "    p_tokens  = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n",
    "    ra_tokens = tokenizer(response_a, add_special_tokens=False)[\"input_ids\"]\n",
    "    rb_tokens = tokenizer(response_b, add_special_tokens=False)[\"input_ids\"]\n",
    "    a_end_token=tokenizer(f\"\"\"\\n</Response_A>\\n{'---'*10}\\n\"\"\", add_special_tokens=False)[\"input_ids\"]\n",
    "    b_end_token=tokenizer(f\"\"\"\\n</Response_B>\\n\\n\"\"\", add_special_tokens=False)[\"input_ids\"]\n",
    "    p_end_token=tokenizer(f\"\"\"\\n</Query>\\n{'---'*10}\\n\"\"\", add_special_tokens=False)[\"input_ids\"]\n",
    "    all_tokens_num = prev_tokens_num +  len(p_tokens) + len(ra_tokens) + len(rb_tokens)+len(a_end_token) + len(b_end_token) + len(p_end_token)\n",
    "    input_ids =[]\n",
    "    if all_tokens_num > max_length:\n",
    "        remain_tokens_num = max_length - prev_tokens_num  - 3*len(dot_tokens) \n",
    "        if remain_tokens_num >5:\n",
    "            p_tokens  =  p_tokens[:int(remain_tokens_num*0.15)] + dot_tokens+ p_tokens[-int(remain_tokens_num*0.05):] if len( p_tokens) > int(remain_tokens_num*0.2) else  p_tokens\n",
    "            ra_tokens = ra_tokens[:int(remain_tokens_num*0.3)] + dot_tokens+ ra_tokens[-int(remain_tokens_num*0.1):] if len(ra_tokens) > int(remain_tokens_num*0.4) else ra_tokens\n",
    "            rb_tokens = rb_tokens[:int(remain_tokens_num*0.3)] + dot_tokens+ rb_tokens[-int(remain_tokens_num*0.1):] if len(rb_tokens) > int(remain_tokens_num*0.4) else rb_tokens\n",
    "            input_ids = p_tokens+p_end_token + ra_tokens +a_end_token+ rb_tokens+b_end_token\n",
    "    else:\n",
    "        prev_tokens_num = all_tokens_num\n",
    "        input_ids = p_tokens+p_end_token + ra_tokens +a_end_token+ rb_tokens+b_end_token\n",
    "    input_ids += final_p_tokens\n",
    "    text=tokenizer.decode(input_ids, skip_special_tokens=False)\n",
    "    conversation = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\":text},\n",
    "        ]\n",
    "    final_prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)+'Answer:\\n'\n",
    "    return final_prompt\n",
    "\n",
    "def apply_template(row, tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": f'''Here is your input to process now-\n",
    "\n",
    "<Query>\n",
    "{row['prompt']}\n",
    "</Query>\n",
    "{'---'*10}\n",
    "<Response_A>\n",
    "{row['response_a'][:6500]}\n",
    "</Response_A>\n",
    "{'---'*10}\n",
    "<Response_B>\n",
    "{row['response_b'][:6500]}\n",
    "</Response_B>\n",
    "\n",
    "Which response is more likely to be selected by a user? (Give the answer and though)\n",
    "Output:\n",
    "'''\n",
    "        }\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    return text\n",
    "\n",
    "model_path = \"/kaggle/input/qwen2.5_14b_final/transformers/14b/1\"\n",
    "\n",
    "offload = 4\n",
    "swap = 1\n",
    "max_len = 12000\n",
    "num_seqs = 256\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "df[\"text\"] = df.apply(lambda row: preprocess_function_truncation(row, tokenizer,max_length=4096,system_prompt=system_prompt,system_prompt_chatformat=system_prompt_chatformat),axis=1)\n",
    "\n",
    "def tok_len(txt):\n",
    "    return len(tokenizer.encode(txt))\n",
    "\n",
    "df['token_length'] = df['text'].apply(tok_len)\n",
    "print(\"max_len\",max(df['token_length']))\n",
    "print(\"Stats of df\",df['token_length'].describe())\n",
    "print('Example input-\\n',df[\"text\"][0])\n",
    "\n",
    "llm = vllm.LLM(model=model_path,\n",
    "    tensor_parallel_size=2,\n",
    "    gpu_memory_utilization=0.99, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\",\n",
    "    enforce_eager=True,\n",
    "    max_model_len=max_len,\n",
    "    disable_log_stats=True,\n",
    "    cpu_offload_gb=offload,\n",
    "    swap_space=swap,\n",
    "    device='cuda',\n",
    "    max_num_seqs=num_seqs,\n",
    "    enable_prefix_caching=True,\n",
    ")\n",
    "\n",
    "responses = llm.generate(\n",
    "    df[\"text\"].values,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  \n",
    "        top_k=1,  \n",
    "        temperature=0,  \n",
    "        skip_special_tokens=False, \n",
    "        max_tokens=1, \n",
    "        logprobs=20,\n",
    "    ),\n",
    "    use_tqdm=True\n",
    ")\n",
    "\n",
    "a_tok_id = tokenizer(\"A\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "b_tok_id = tokenizer(\"B\", add_special_tokens=False)[\"input_ids\"][-1]\n",
    "\n",
    "print(f\">> EediRanker: A token id: {a_tok_id}\")\n",
    "print(f\">> EediRanker: B token id: {b_tok_id}\")\n",
    "\n",
    "result = []\n",
    "n = 0\n",
    "for idx, response in enumerate(responses):\n",
    "    try:\n",
    "        logprob_dict = response.outputs[0].logprobs[0]\n",
    "        n += 1\n",
    "        top_tok_ids = set(list(logprob_dict.keys()))\n",
    "        \n",
    "        if len(top_tok_ids.intersection(set([a_tok_id, b_tok_id]))) == 0:\n",
    "            print(f\"Bad Output for {n}\")\n",
    "            result.append('A')\n",
    "            continue\n",
    "            \n",
    "        # Use the new winner determination function\n",
    "        winner_choice = winner_with_postprocessing(\n",
    "            logprob_dict,\n",
    "            df.iloc[idx],\n",
    "            a_tok_id,\n",
    "            b_tok_id,\n",
    "            threshold=0.005\n",
    "        )\n",
    "        result.append(winner_choice)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response {n}: {str(e)}\")\n",
    "        result.append('A')\n",
    "\n",
    "print('Raw responses: ', result)\n",
    "df[\"winner\"] = [f'model_{x.lower()}' for x in result]\n",
    "\n",
    "if IS_SUBMISSION:\n",
    "    df.to_csv(\"submission.csv\", columns=[\"id\", \"winner\"], index=False)\n",
    "else:\n",
    "    df.to_csv(\"submission.csv\", columns=[\"id\", \"winner\", \"winner_GT\"], index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffd6f5cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T16:02:26.129078Z",
     "iopub.status.busy": "2025-02-03T16:02:26.128817Z",
     "iopub.status.idle": "2025-02-03T16:08:46.484093Z",
     "shell.execute_reply": "2025-02-03T16:08:46.482885Z"
    },
    "papermill": {
     "duration": 380.361327,
     "end_time": "2025-02-03T16:08:46.486294",
     "exception": false,
     "start_time": "2025-02-03T16:02:26.124967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df= 100\r\n",
      "max_len 4002\r\n",
      "Stats of df count     100.000000\r\n",
      "mean     1391.960000\r\n",
      "std       918.510354\r\n",
      "min       284.000000\r\n",
      "25%       755.000000\r\n",
      "50%      1188.000000\r\n",
      "75%      1793.750000\r\n",
      "max      4002.000000\r\n",
      "Name: token_length, dtype: float64\r\n",
      "Example input-\r\n",
      " <|im_start|>system\r\n",
      "You are a skilled judge evaluating responses from two large language models(LLMs). Your task is to select the response that best meets the user's needs based on the query provided.\r\n",
      "\r\n",
      "**Input Format:**\r\n",
      "<Query>\r\n",
      "[User's original query to both LLMs]\r\n",
      "</Query>\r\n",
      "\r\n",
      "<Response_A>\r\n",
      "[First LLM's response]\r\n",
      "</Response_A>\r\n",
      "\r\n",
      "<Response_B>\r\n",
      "[Second LLM's response]\r\n",
      "</Response_B>\r\n",
      "\r\n",
      "**Your Task:**\r\n",
      "Carefully analyze both <Response_A> and <Response_B> in relation to the Query. Determine which response is more likely to be selected by a user based on the following criteria:\r\n",
      "- Completeness in addressing the query\r\n",
      "- Accuracy of information\r\n",
      "- Clarity and coherence\r\n",
      "- Conciseness vs appropriate detail\r\n",
      "- Helpful examples or explanations when needed\r\n",
      "- Professional yet engaging tone\r\n",
      "- Sound reasoning and logic\r\n",
      "- Format and presentation\r\n",
      "\r\n",
      "<|im_end|>\r\n",
      "<|im_start|>user\r\n",
      "**Here is your input to process now-**\r\n",
      "Input:\r\n",
      "\r\n",
      "<Query>\r\n",
      "c++如何实现快速幂\r\n",
      "</Query>\r\n",
      "------------------------------\r\n",
      "\r\n",
      "<Response_A>\r\n",
      "```cpp\r\n",
      "#include <iostream>\r\n",
      "\r\n",
      "using namespace std;\r\n",
      "\r\n",
      "// 快速幂算法\r\n",
      "long long fast_power(long long base, long long exponent, long long modulus) {\r\n",
      "  long long result = 1;\r\n",
      "  while (exponent > 0) {\r\n",
      "    if (exponent % 2 == 1) {\r\n",
      "      result = (result * base) % modulus;\r\n",
      "    }\r\n",
      "    base = (base * base) % modulus;\r\n",
      "    exponent /= 2;\r\n",
      "  }\r\n",
      "  return result;\r\n",
      "}\r\n",
      "\r\n",
      "int main() {\r\n",
      "  long long base, exponent, modulus;\r\n",
      "\r\n",
      "  cout << \"Enter base: \";\r\n",
      "  cin >> base;\r\n",
      "\r\n",
      "  cout << \"Enter exponent: \";\r\n",
      "  cin >> exponent;\r\n",
      "\r\n",
      "  cout << \"Enter modulus: \";\r\n",
      "  cin >> modulus;\r\n",
      "\r\n",
      "  long long result = fast_power(base, exponent, modulus);\r\n",
      "\r\n",
      "  cout << base << \"^\" << exponent << \" % \" << modulus << \" = \" << result << endl;\r\n",
      "\r\n",
      "  return 0;\r\n",
      "}\r\n",
      "```\r\n",
      "\r\n",
      "**解释:**\r\n",
      "\r\n",
      "1. **函数定义:**\r\n",
      "   - `fast_power(long long base, long long exponent, long long modulus)`: 接受底数、指数和模数作为参数，返回结果。\r\n",
      "\r\n",
      "2. **初始化:**\r\n",
      "   - `result = 1`: 初始化结果为 1。\r\n",
      "\r\n",
      "3. **循环:**\r\n",
      "   - `while (exponent > 0)`: 循环直到指数为 0。\r\n",
      "   - `if (exponent % 2 == 1)`: 如果指数为奇数，将结果乘以当前底数并取模。\r\n",
      "   - `base = (base * base) % modulus`: 将底数平方并取模，准备下一轮循环。\r\n",
      "   - `exponent /= 2`: 将指数除以 2。\r\n",
      "\r\n",
      "4. **返回结果:**\r\n",
      "   - `return result`: 返回计算结果。\r\n",
      "\r\n",
      "**原理:**\r\n",
      "\r\n",
      "快速幂算法利用二进制表示法来优化幂运算。它将指数转换为二进制形式，然后根据二进制位的值进行乘法操作。\r\n",
      "\r\n",
      "例如，计算 2^13：\r\n",
      "\r\n",
      "1. 13 的二进制表示为 1101。\r\n",
      "2. 从右到左遍历二进制位：\r\n",
      "   - 第一位为 1，所以结果乘以 2^1 = 2。\r\n",
      "   - 第二位为 0，不进行操作。\r\n",
      "   - 第三位为 1，所以结果乘以 2^2 = 4。\r\n",
      "   - 第四位为 1，所以结果乘以 2^4 = 16。\r\n",
      "\r\n",
      "最终结果为 2 * 4 * 16 = 128。\r\n",
      "\r\n",
      "**优点:**\r\n",
      "\r\n",
      "- 时间复杂度为 O(log n)，比朴素的 O(n) 算法效率更高。\r\n",
      "\r\n",
      "**应用:**\r\n",
      "\r\n",
      "- 模运算\r\n",
      "- RSA 加密算法\r\n",
      "- 快速矩阵乘法\r\n",
      "</Response_A>\r\n",
      "------------------------------\r\n",
      "\r\n",
      "<Response_B>\r\n",
      "在C++中，实现快速幂（也称为快速幂取模）可以通过二分法来优化幂运算的效率，尤其是在处理大数时非常有用。快速幂算法将幂运算的时间复杂度从 \\(O(n)\\) 降低到了 \\(O(\\log n)\\)。以下是如何实现这个算法的步骤：\r\n",
      "\r\n",
      "### 步骤\r\n",
      "\r\n",
      "1. **基本思路**：利用幂的性质`(a * b) % n = ((a % n) * (b % n)) % n`。通过将指数二分，可以减少乘法次数。\r\n",
      "\r\n",
      "2. **实现步骤**：\r\n",
      "   - 如果指数为0，返回1（任何非零数的0次幂都是1）。\r\n",
      "   - 如果指数是偶数，将问题转化为求`(a^(n/2))^2 % m`。\r\n",
      "   - 如果指数是奇数，先计算`(a * (a^(n-1))) % m`，其中`(a^(n-1))`继续用快速幂法求解。\r\n",
      "\r\n",
      "### 代码实现\r\n",
      "\r\n",
      "```cpp\r\n",
      "#include <iostream>\r\n",
      "\r\n",
      "class Solution {\r\n",
      "public:\r\n",
      "    // 快速幂取模函数\r\n",
      "    long long quickPowMod(long long a, long long n, long long m) {\r\n",
      "        // 结果初始化为1，因为任何数的0次方是1\r\n",
      "        long long res = 1;\r\n",
      "        a = a % m; // 更新a为a对m取模的结果，防止a过大\r\n",
      "        \r\n",
      "        while (n > 0) {\r\n",
      "            // 如果n是奇数\r\n",
      "            if (n & 1) {\r\n",
      "                res = (res * a) % m; // 乘上当前的a\r\n",
      "            }\r\n",
      "            // n 现在一定是偶数\r\n",
      "            n = n >> 1; // n 除以 2\r\n",
      "            a = (a * a) % m; // 平方\r\n",
      "        }\r\n",
      "        return res;\r\n",
      "    }\r\n",
      "};\r\n",
      "\r\n",
      "int main() {\r\n",
      "    Solution sol;\r\n",
      "    long long base = 2;\r\n",
      "    long long exponent = 10;\r\n",
      "    long long modulus = 1000000007; // 常用的大素数，用于取模运算\r\n",
      "    \r\n",
      "    std::cout << \"2^10 mod 1000000007 = \" << sol.quickPowMod(base, exponent, modulus) << std::endl;\r\n",
      "    return 0;\r\n",
      "}\r\n",
      "```\r\n",
      "\r\n",
      "### 说明\r\n",
      "\r\n",
      "- **long long**: 使用`long long`类型以处理可能出现的大数。\r\n",
      "- **取模操作**: 在每一步都进行取模操作是为了防止整数溢出，尤其是在处理非常大的数时。\r\n",
      "- **位运算**: `n & 1` 判断 n 是否为奇数，`n >> 1` 相当于 `n / 2`，这是利用位运算加速的技巧。\r\n",
      "\r\n",
      "这个算法不仅在竞赛编程中非常有用，在密码学等需要处理大数运算的领域也广泛应用。\r\n",
      "</Response_B>\r\n",
      "\r\n",
      "Which response is more likely to be selected by a user? (A or B)\r\n",
      "<|im_end|>\r\n",
      "<|im_start|>assistant\r\n",
      "Answer:\r\n",
      "\r\n",
      "WARNING 02-03 16:02:39 config.py:1668] Casting torch.bfloat16 to torch.float16.\r\n",
      "INFO 02-03 16:02:48 config.py:905] Defaulting to use mp for distributed inference\r\n",
      "WARNING 02-03 16:02:48 config.py:395] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 02-03 16:02:48 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='/kaggle/input/qwen2.5_14b_final/transformers/14b/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5_14b_final/transformers/14b/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=12000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/kaggle/input/qwen2.5_14b_final/transformers/14b/1, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=True, use_async_output_proc=False, use_cached_outputs=False, mm_processor_kwargs=None)\r\n",
      "WARNING 02-03 16:02:49 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 02-03 16:02:49 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\r\n",
      "  self.pid = os.fork()\r\n",
      "INFO 02-03 16:02:49 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 02-03 16:02:49 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:02:49 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:02:49 selector.py:115] Using XFormers backend.\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n",
      "/opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m /opt/conda/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:02:50 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:02:51 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "INFO 02-03 16:02:51 utils.py:1008] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:02:51 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 02-03 16:02:51 pynccl.py:63] vLLM is using nccl==2.20.5\r\n",
      "INFO 02-03 16:02:51 custom_all_reduce_utils.py:204] generating GPU P2P access cache in /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 02-03 16:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:03:09 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\r\n",
      "INFO 02-03 16:03:09 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x79a569e7e1d0>, local_subscribe_port=45303, remote_subscribe_port=None)\r\n",
      "INFO 02-03 16:03:09 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5_14b_final/transformers/14b/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:03:09 model_runner.py:1056] Starting to load model /kaggle/input/qwen2.5_14b_final/transformers/14b/1...\r\n",
      "INFO 02-03 16:03:09 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 02-03 16:03:09 selector.py:115] Using XFormers backend.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:03:09 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:03:09 selector.py:115] Using XFormers backend.\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/6 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  17% Completed | 1/6 [00:27<02:17, 27.59s/it]\r\n",
      "Loading safetensors checkpoint shards:  33% Completed | 2/6 [00:59<02:00, 30.05s/it]\r\n",
      "Loading safetensors checkpoint shards:  50% Completed | 3/6 [01:27<01:27, 29.18s/it]\r\n",
      "Loading safetensors checkpoint shards:  67% Completed | 4/6 [01:58<00:59, 29.80s/it]\r\n",
      "Loading safetensors checkpoint shards:  83% Completed | 5/6 [02:30<00:30, 30.72s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [02:59<00:00, 30.08s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 6/6 [02:59<00:00, 29.91s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=144)\u001b[0;0m INFO 02-03 16:06:16 model_runner.py:1067] Loading model weights took 9.8378 GB\r\n",
      "INFO 02-03 16:06:16 model_runner.py:1067] Loading model weights took 9.8378 GB\r\n",
      "INFO 02-03 16:06:27 distributed_gpu_executor.py:57] # GPU blocks: 1136, # CPU blocks: 682\r\n",
      "INFO 02-03 16:06:27 distributed_gpu_executor.py:61] Maximum concurrency for 12000 tokens per request: 1.51x\r\n",
      "Processed prompts: 100%|█| 100/100 [02:06<00:00,  1.26s/it, est. speed input: 11\r\n",
      ">> EediRanker: A token id: 32\r\n",
      ">> EediRanker: B token id: 33\r\n",
      "Raw responses:  ['B', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'B', 'A', 'B', 'B', 'A', 'A', 'B', 'B', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'A', 'B', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'B', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'B', 'A', 'B', 'A', 'B', 'A', 'B', 'B', 'B', 'A', 'B', 'A', 'B', 'B', 'A', 'A']\r\n",
      "[rank0]:[W203 16:08:42.640031513 CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]\r\n"
     ]
    }
   ],
   "source": [
    "# if IS_SUBMISSION or run_eval:\n",
    "!python run_vllm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7135456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T16:08:46.499681Z",
     "iopub.status.busy": "2025-02-03T16:08:46.499347Z",
     "iopub.status.idle": "2025-02-03T16:08:46.517362Z",
     "shell.execute_reply": "2025-02-03T16:08:46.516452Z"
    },
    "papermill": {
     "duration": 0.026998,
     "end_time": "2025-02-03T16:08:46.518910",
     "exception": false,
     "start_time": "2025-02-03T16:08:46.491912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "if not IS_SUBMISSION and run_eval:\n",
    "    df = pd.read_csv('submission.csv')\n",
    "    correct_preds = (df['winner'] == df['winner_GT']).sum()\n",
    "    total_preds = len(df)\n",
    "    acc = correct_preds / total_preds\n",
    "    \n",
    "    print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2777f9a",
   "metadata": {
    "papermill": {
     "duration": 0.005191,
     "end_time": "2025-02-03T16:08:46.529425",
     "exception": false,
     "start_time": "2025-02-03T16:08:46.524234",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10131489,
     "sourceId": 86946,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6117312,
     "sourceId": 9948011,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6388842,
     "sourceId": 10348841,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6457437,
     "sourceId": 10418963,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6457628,
     "sourceId": 10419223,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6486049,
     "sourceId": 10475031,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6459450,
     "sourceId": 10504525,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6521895,
     "sourceId": 10557485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6540368,
     "isSourceIdPinned": true,
     "sourceId": 10583171,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 210357319,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 121027,
     "modelInstanceId": 100936,
     "sourceId": 120005,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 154149,
     "modelInstanceId": 131339,
     "sourceId": 154590,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141469,
     "sourceId": 166258,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141475,
     "sourceId": 166264,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 141552,
     "sourceId": 166355,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 145960,
     "sourceId": 171496,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 164048,
     "modelInstanceId": 145962,
     "sourceId": 171498,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182744,
     "modelInstanceId": 160376,
     "sourceId": 188115,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 182769,
     "modelInstanceId": 160415,
     "sourceId": 188158,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 185749,
     "modelInstanceId": 163393,
     "sourceId": 191689,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 234303,
     "modelInstanceId": 212657,
     "sourceId": 248791,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 553.630018,
   "end_time": "2025-02-03T16:08:48.659091",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-03T15:59:35.029073",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
